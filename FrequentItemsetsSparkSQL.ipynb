{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class item\n",
       "defined class candidate\n",
       "dataRDD = MapPartitionsRDD[3] at map at <console>:29\n",
       "dataCount = 100000\n",
       "support = 1000.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "import org.apache.spark.sql.functions.collect_list\n",
    "import org.apache.spark.sql.functions.count\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "\n",
    "case class item(items: Array[Int])\n",
    "case class candidate(value: Array[Int])\n",
    "\n",
    "val dataRDD = sc.textFile(\"T10I4D100K.dat\").map(row => row.split(\" \").map(v => v.toInt).toArray).map(d => item(d))\n",
    "val dataCount = dataRDD.count()\n",
    "val support = 0.01 * dataCount\n",
    "\n",
    "//rawDataRDD.collect.foreach(println)\n",
    "//println(dataCount)\n",
    "//println(support)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val itemsRDD = dataRDD.zipWithIndex().map(d => item(d._2, d._1))\n",
    "//println(itemsRDD.take(1)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidates = MapPartitionsRDD[9] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[9] at map at <console>:47"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidates = dataRDD.map(row => row.items.map(item => (item) ))\n",
    "    .flatMap(y => y)\n",
    "    .distinct\n",
    "    .map(c => candidate(Array(c)))\n",
    "println(candidates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| [68]|\n",
      "|[454]|\n",
      "|[324]|\n",
      "|[180]|\n",
      "|[320]|\n",
      "|[752]|\n",
      "|[408]|\n",
      "|[428]|\n",
      "|[986]|\n",
      "|[464]|\n",
      "|[346]|\n",
      "| [14]|\n",
      "|[466]|\n",
      "| [24]|\n",
      "|[520]|\n",
      "|[912]|\n",
      "|[146]|\n",
      "|[140]|\n",
      "|[204]|\n",
      "|[514]|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|               items|value|\n",
      "+--------------------+-----+\n",
      "|[25, 52, 164, 240...| [68]|\n",
      "|[25, 52, 164, 240...|[454]|\n",
      "|[25, 52, 164, 240...|[324]|\n",
      "|[25, 52, 164, 240...|[180]|\n",
      "|[25, 52, 164, 240...|[320]|\n",
      "|[25, 52, 164, 240...|[752]|\n",
      "|[25, 52, 164, 240...|[408]|\n",
      "|[25, 52, 164, 240...|[428]|\n",
      "|[25, 52, 164, 240...|[986]|\n",
      "|[25, 52, 164, 240...|[464]|\n",
      "|[25, 52, 164, 240...|[346]|\n",
      "|[25, 52, 164, 240...| [14]|\n",
      "|[25, 52, 164, 240...|[466]|\n",
      "|[25, 52, 164, 240...| [24]|\n",
      "|[25, 52, 164, 240...|[520]|\n",
      "|[25, 52, 164, 240...|[912]|\n",
      "|[25, 52, 164, 240...|[146]|\n",
      "|[25, 52, 164, 240...|[140]|\n",
      "|[25, 52, 164, 240...|[204]|\n",
      "|[25, 52, 164, 240...|[514]|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+---------+\n",
      "|               items|value|singleton|\n",
      "+--------------------+-----+---------+\n",
      "|[25, 52, 164, 240...| [68]|       []|\n",
      "|[25, 52, 164, 240...|[454]|       []|\n",
      "|[25, 52, 164, 240...|[324]|       []|\n",
      "|[25, 52, 164, 240...|[180]|       []|\n",
      "|[25, 52, 164, 240...|[320]|       []|\n",
      "|[25, 52, 164, 240...|[752]|       []|\n",
      "|[25, 52, 164, 240...|[408]|       []|\n",
      "|[25, 52, 164, 240...|[428]|       []|\n",
      "|[25, 52, 164, 240...|[986]|       []|\n",
      "|[25, 52, 164, 240...|[464]|       []|\n",
      "|[25, 52, 164, 240...|[346]|       []|\n",
      "|[25, 52, 164, 240...| [14]|       []|\n",
      "|[25, 52, 164, 240...|[466]|       []|\n",
      "|[25, 52, 164, 240...| [24]|       []|\n",
      "|[25, 52, 164, 240...|[520]|       []|\n",
      "|[25, 52, 164, 240...|[912]|       []|\n",
      "|[25, 52, 164, 240...|[146]|       []|\n",
      "|[25, 52, 164, 240...|[140]|       []|\n",
      "|[25, 52, 164, 240...|[204]|       []|\n",
      "|[25, 52, 164, 240...|[514]|       []|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|               items|singleton|\n",
      "+--------------------+---------+\n",
      "|[25, 52, 164, 240...|    [834]|\n",
      "|[25, 52, 164, 240...|    [164]|\n",
      "|[25, 52, 164, 240...|    [368]|\n",
      "|[25, 52, 164, 240...|    [730]|\n",
      "|[25, 52, 164, 240...|     [52]|\n",
      "|[25, 52, 164, 240...|    [240]|\n",
      "|[25, 52, 164, 240...|    [630]|\n",
      "|[25, 52, 164, 240...|    [274]|\n",
      "|[25, 52, 164, 240...|    [538]|\n",
      "|[25, 52, 164, 240...|    [328]|\n",
      "|[25, 52, 164, 240...|    [448]|\n",
      "|[39, 120, 124, 20...|    [814]|\n",
      "|[39, 120, 124, 20...|    [834]|\n",
      "|[39, 120, 124, 20...|    [124]|\n",
      "|[39, 120, 124, 20...|    [120]|\n",
      "|[39, 120, 124, 20...|    [704]|\n",
      "|[35, 249, 674, 71...|    [712]|\n",
      "|[35, 249, 674, 71...|    [854]|\n",
      "|[35, 249, 674, 71...|    [674]|\n",
      "|[35, 249, 674, 71...|    [950]|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+\n",
      "|singleton|support|\n",
      "+---------+-------+\n",
      "|    [496]|   1428|\n",
      "|    [471]|   2894|\n",
      "|    [392]|   2420|\n",
      "|    [540]|   1293|\n",
      "|    [623]|   1845|\n",
      "|    [897]|   1935|\n",
      "|    [516]|   1544|\n",
      "|     [31]|   1666|\n",
      "|    [580]|   1667|\n",
      "|     [85]|   1555|\n",
      "|    [458]|   1124|\n",
      "|    [883]|   4902|\n",
      "|    [970]|   2086|\n",
      "|    [804]|   1315|\n",
      "|    [296]|   2210|\n",
      "|    [472]|   2125|\n",
      "|    [853]|   1804|\n",
      "|     [78]|   2471|\n",
      "|    [918]|   3012|\n",
      "|    [322]|   1154|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidatesDF = [value: array<int>]\n",
       "dataDF = [items: array<int>]\n",
       "crossDF = [items: array<int>, value: array<int>]\n",
       "singletonDF = [items: array<int>, value: array<int> ... 1 more field]\n",
       "sqlContext = org.apache.spark.sql.SQLContext@5960b6fa\n",
       "frequent_singletons = [items: array<int>, singleton: array<int>]\n",
       "freq_singletons = [singleton: array<int>, support: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[singleton: array<int>, support: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidatesDF = candidates.toDF()\n",
    "val dataDF = dataRDD.toDF()\n",
    "val crossDF = dataDF.crossJoin(candidatesDF)\n",
    "candidatesDF.show()\n",
    "crossDF.show()\n",
    "\n",
    "val singletonDF = crossDF.withColumn(\n",
    "    \"singleton\",\n",
    "    array_intersect( col(\"value\"), col(\"items\"))\n",
    ")\n",
    "singletonDF.show()\n",
    "singletonDF.createOrReplaceTempView(\"singletons\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val frequent_singletons = sqlContext.sql(\"SELECT d.items, d.singleton FROM singletons d WHERE size(d.singleton) > 0 \")\n",
    "frequent_singletons.show()\n",
    "\n",
    "val freq_singletons = frequent_singletons.groupBy(\"singleton\").agg(count(\"items\")as \"support\").where(col(\"support\") > support)\n",
    "freq_singletons.show\n",
    "\n",
    "//println(dup_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|singleton|singleton2|\n",
      "+---------+----------+\n",
      "|    [471]|     [496]|\n",
      "|    [496]|     [540]|\n",
      "|    [496]|     [623]|\n",
      "|    [496]|     [897]|\n",
      "|    [471]|     [540]|\n",
      "|    [471]|     [623]|\n",
      "|    [471]|     [897]|\n",
      "|    [496]|     [516]|\n",
      "|    [471]|     [516]|\n",
      "|    [496]|     [580]|\n",
      "|    [471]|     [580]|\n",
      "|    [496]|     [883]|\n",
      "|    [471]|     [883]|\n",
      "|    [496]|     [970]|\n",
      "|    [496]|     [804]|\n",
      "|    [471]|     [970]|\n",
      "|    [471]|     [804]|\n",
      "|    [496]|     [853]|\n",
      "|    [471]|     [472]|\n",
      "|    [471]|     [853]|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+\n",
      "|candidate2|               items|\n",
      "+----------+--------------------+\n",
      "|[471, 496]|[25, 52, 164, 240...|\n",
      "|[471, 496]|[39, 120, 124, 20...|\n",
      "|[471, 496]|[35, 249, 674, 71...|\n",
      "|[471, 496]|[39, 422, 449, 70...|\n",
      "|[471, 496]|[15, 229, 262, 28...|\n",
      "|[471, 496]|[26, 104, 143, 32...|\n",
      "|[471, 496]|[7, 185, 214, 350...|\n",
      "|[471, 496]|          [227, 390]|\n",
      "|[471, 496]|[71, 192, 208, 27...|\n",
      "|[471, 496]|[183, 193, 217, 2...|\n",
      "|[471, 496]|[161, 175, 177, 4...|\n",
      "|[471, 496]|[125, 130, 327, 6...|\n",
      "|[471, 496]|[392, 461, 569, 8...|\n",
      "|[471, 496]|[27, 78, 104, 177...|\n",
      "|[471, 496]|[101, 147, 229, 3...|\n",
      "|[471, 496]|[71, 208, 217, 26...|\n",
      "|[471, 496]|[43, 70, 176, 204...|\n",
      "|[471, 496]|  [25, 52, 278, 730]|\n",
      "|[471, 496]|[151, 432, 504, 8...|\n",
      "|[471, 496]|[71, 73, 118, 274...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidate_2a = [singleton: array<int>]\n",
       "candidate_2 = [singleton: array<int>, singleton2: array<int>]\n",
       "crossDF2 = [candidate2: array<int>, items: array<int>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[candidate2: array<int>, items: array<int>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidate_2a = freq_singletons.select(\"singleton\")\n",
    "val candidate_2 = candidate_2a.crossJoin(candidate_2a.select(col(\"singleton\") as \"singleton2\")).where($\"singleton\" < $\"singleton2\")\n",
    "\n",
    "candidate_2.show\n",
    "\n",
    "val crossDF2 = candidate_2.withColumn(\n",
    "    \"candidate2\",\n",
    "    concat(col(\"singleton\"), col(\"singleton2\"))\n",
    ").crossJoin(dataDF).drop(\"singleton\").drop(\"singleton2\")\n",
    "crossDF2.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+\n",
      "|candidate2|               items|dupleton|\n",
      "+----------+--------------------+--------+\n",
      "|[471, 496]|[25, 52, 164, 240...|       0|\n",
      "|[471, 496]|[39, 120, 124, 20...|       0|\n",
      "|[471, 496]|[35, 249, 674, 71...|       0|\n",
      "|[471, 496]|[39, 422, 449, 70...|       0|\n",
      "|[471, 496]|[15, 229, 262, 28...|       0|\n",
      "|[471, 496]|[26, 104, 143, 32...|       0|\n",
      "|[471, 496]|[7, 185, 214, 350...|       0|\n",
      "|[471, 496]|          [227, 390]|       0|\n",
      "|[471, 496]|[71, 192, 208, 27...|       1|\n",
      "|[471, 496]|[183, 193, 217, 2...|       1|\n",
      "|[471, 496]|[161, 175, 177, 4...|       0|\n",
      "|[471, 496]|[125, 130, 327, 6...|       0|\n",
      "|[471, 496]|[392, 461, 569, 8...|       0|\n",
      "|[471, 496]|[27, 78, 104, 177...|       0|\n",
      "|[471, 496]|[101, 147, 229, 3...|       0|\n",
      "|[471, 496]|[71, 208, 217, 26...|       0|\n",
      "|[471, 496]|[43, 70, 176, 204...|       0|\n",
      "|[471, 496]|  [25, 52, 278, 730]|       0|\n",
      "|[471, 496]|[151, 432, 504, 8...|       0|\n",
      "|[471, 496]|[71, 73, 118, 274...|       0|\n",
      "+----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dupletonDF = [candidate2: array<int>, items: array<int> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "contains_all: (x: org.apache.spark.sql.Column, y: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[candidate2: array<int>, items: array<int> ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "def contains_all(x: Column, y: Column): Column = {\n",
    "    return size(array_intersect(x, y))\n",
    "}\n",
    "val dupletonDF = crossDF2.withColumn(\n",
    "    \"dupleton\",\n",
    "    contains_all( col(\"candidate2\"), col(\"items\"))\n",
    ")\n",
    "dupletonDF.show()\n",
    "dupletonDF.createOrReplaceTempView(\"dupletons\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+\n",
      "|               items|candidate2|dupleton|\n",
      "+--------------------+----------+--------+\n",
      "|[8, 78, 427, 450,...|[471, 496]|       2|\n",
      "|[6, 368, 471, 475...|[471, 496]|       2|\n",
      "|[185, 471, 496, 6...|[471, 496]|       2|\n",
      "|[90, 185, 239, 24...|[471, 496]|       2|\n",
      "|[97, 100, 112, 12...|[471, 496]|       2|\n",
      "|[177, 197, 242, 3...|[471, 496]|       2|\n",
      "|[71, 145, 172, 27...|[471, 496]|       2|\n",
      "|[68, 145, 217, 24...|[471, 496]|       2|\n",
      "|[64, 95, 242, 307...|[471, 496]|       2|\n",
      "|[68, 217, 334, 34...|[471, 496]|       2|\n",
      "|[39, 177, 471, 49...|[471, 496]|       2|\n",
      "|[242, 392, 461, 4...|[471, 496]|       2|\n",
      "|[142, 145, 171, 2...|[471, 496]|       2|\n",
      "|[132, 161, 242, 2...|[471, 496]|       2|\n",
      "|[101, 114, 145, 1...|[471, 496]|       2|\n",
      "|[32, 112, 181, 24...|[471, 496]|       2|\n",
      "|[368, 471, 496, 5...|[471, 496]|       2|\n",
      "|[27, 89, 177, 198...|[471, 496]|       2|\n",
      "|[110, 355, 438, 4...|[471, 496]|       2|\n",
      "|[18, 129, 185, 23...|[471, 496]|       2|\n",
      "+--------------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@70eb1a56\n",
       "frequent_dupl = [items: array<int>, candidate2: array<int> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[items: array<int>, candidate2: array<int> ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new SQLContext(sc)\n",
    "val frequent_dupl = sqlContext.sql(\"SELECT d.items, d.candidate2, d.dupleton FROM dupletons d WHERE d.dupleton == 2\")\n",
    "frequent_dupl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val freq_dupletons = frequent_dupl.groupBy(\"candidate2\").agg(count(\"*\")as \"support\").where(col(\"support\") > support)\n",
    "//freq_dupletons.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job 37 cancelled as part of cancellation of all jobs\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2082)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class duples(items: Array[Int], candidate2:Array[Int], support: Int)\n",
    "val dupleRDD = frequent_dupl.rdd.map(r => (r(0), 1))\n",
    "    .reduceByKey(_ + _)\n",
    "dupleRDD.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//archive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//k = 1\n",
    "val k = 1\n",
    "val singleton_c = dataRDD.map(r=> r.map(item => (item, 1)))\n",
    "    .flatMap(y=>y).reduceByKey(_ + _)\n",
    "    .map(d => candidate(d._1, d._2))\n",
    "val singleton = singleton_c.filter(s => s.support >= support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val k_ = 2\n",
    "//val dupletons = singleton.map(s => s.value).collect.toSet.subsets(k_).copyToArray(d2)\n",
    "case class candidate2(value: Array[Int], support: Int)\n",
    "val candidat = singleton.map(s => candidate2(Array(s.value), s.support))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val singletonDF = spark.createDataFrame(singleton) \n",
    "val itemsDF = spark.createDataFrame(itemsRDD)\n",
    "//itemsDF.show(200)\n",
    "\n",
    "singletonDF.createOrReplaceTempView(\"single\")\n",
    "itemsDF.createOrReplaceTempView(\"data\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val dup = sqlContext.sql(\"SELECT s1.value as v1, s2.value as v2, d.items, d.row FROM data d, single s1, single s2 WHERE s1.value != s2.value AND array_contains(d.items, s1.value) AND array_contains(d.items, s2.value)\")\n",
    "dup.show()\n",
    "println(dup.count())\n",
    "\n",
    "\n",
    "val result = dup.rdd.map(d => ((d(0), d(1)), 1)).reduceByKey(_ + _).filter(d => d._2 > support)\n",
    "result.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "\n",
    "val singleDF = spark.createDataFrame(candidat) \n",
    "singleDF.show()\n",
    "\n",
    "val crossDF = itemsDF.crossJoin(singleDF)\n",
    "crossDF.show()\n",
    "\n",
    "val dupleDF = crossDF.withColumn(\n",
    "    \"singleton\",\n",
    "    array_intersect( col(\"value\"), col(\"items\"))\n",
    ")\n",
    "dupleDF.show()\n",
    "dupleDF.createOrReplaceTempView(\"dupz\")\n",
    "val dup_filtered = sqlContext.sql(\"SELECT d.items, d.row, d.singleton FROM dupz d WHERE size(d.singleton) > 0 \")\n",
    "dup_filtered.show()\n",
    "\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "dup_filtered.groupBy(\"singleton\").agg(count(\"row\")as \"support\").where(col(\"support\") > support).show\n",
    "\n",
    "//println(dup_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCandidates(row: Set[Int], candidates: Set[Int], k: Int): Set[(Set[Int],Int)] = {\n",
    "    val possible = candidates.subsets(k)\n",
    "    val c = possible.map(p => if(p.subsetOf(row)) (p,1) else (p,0) ).toSet\n",
    "    c\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "val data = dataRDD.map(d => generateCandidates(d, candidates, 2))\n",
    "data.collect.foreach(println)\n",
    "//apriori_k(1, support, candidates, dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_k(k: Int, s: Float, candidates: RDD[Int], data: Any): Any = {\n",
    "    if( candidates.size < 2) return data\n",
    "    \n",
    "    C_t = generateCandidates(k, candidates, s)\n",
    "    println(k)\n",
    "    apriori_k(k+1, s, sc.parallelize(candidates.take((candidates.count()-1).toInt)), data ++ C_t)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
