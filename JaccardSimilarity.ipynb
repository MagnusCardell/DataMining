{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileName1 = dataset_abstracts/a9000006.txt\n",
       "fileName2 = dataset_abstracts/a9000031.txt\n",
       "rows1 = MapPartitionsRDD[23] at map at <console>:36\n",
       "text1 = Commercial exploitation over the past two hundred years drovethe great Mysticete whales to near extinction. Variation inthe sizes of populations prior to exploitation, minimalpopulation size during exploitation and current populationsizes permit analyses of the effects of differing levels ofexploitation on species with different biogeographicaldistributions and life-history characteristics. Dr. StephenPalumbi at the University of Hawaii will study the geneticpopulation structure of three whale species in this context,the Humpback Whale, the Gray Whale and the Bowhead Whale. Theeffect of...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Commercial exploitation over the past two hundred years drovethe great Mysticete whales to near extinction. Variation inthe sizes of populations prior to exploitation, minimalpopulation size during exploitation and current populationsizes permit analyses of the effects of differing levels ofexploitation on species with different biogeographicaldistributions and life-history characteristics. Dr. StephenPalumbi at the University of Hawaii will study the geneticpopulation structure of three whale species in this context,the Humpback Whale, the Gray Whale and the Bowhead Whale. Theeffect of..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//input\n",
    "val fileName1 = \"dataset_abstracts/a9000006.txt\"\n",
    "val fileName2 = \"dataset_abstracts/a9000031.txt\"\n",
    "\n",
    "val rows1 = sc.textFile(fileName1).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text1 = rows1.reduce(_ + _)\n",
    "\n",
    "val rows2 = sc.textFile(fileName2).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text2 = rows2.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k = 5\n",
       "k_gram1 = List((-368554769,\"\"), (-2116654235,\"\"), (1348093184,\"\"), (-713942165,\"\"), (154171951,\"\"), (-1539835203,\"\"), (621014717,\"\"), (-702057327,\"\"), (1061980120,\"\"), (-1210501041,\"\"), (-1777376094,\"\"), (-2146599456,\"\"), (1638764184,\"\"), (2132630876,\"\"), (-558803310,\"\"), (-1704996955,\"\"), (1554848983,\"\"), (-643897612,\"\"), (277879516,\"\"), (1658643371,\"\"), (-239693190,\"\"), (-1096516904,\"\"), (1369629706,\"\"), (2053951917,\"\"), (1404211726,\"\"), (-496476476,\"\"), (66848233,\"\"), (-297738899,\"\"), (1003038978,\"\"), (229228928,\"\"), (-120662840,\"\"), (1068215916,\"\"), (-475916955,\"\"), (-443907787,\"\"), (2075776880,\"\"), (230583223,\"\"), (1330534066,\"\"), (-1120787669,\"\"), (-1584074738,\"\"), (-1702092513,\"\"), (1354781654,\"\"), (-1494390031,\"\"), (-310258093,\"\"), (1911875298,\"\")...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((-368554769,\"\"), (-2116654235,\"\"), (1348093184,\"\"), (-713942165,\"\"), (154171951,\"\"), (-1539835203,\"\"), (621014717,\"\"), (-702057327,\"\"), (1061980120,\"\"), (-1210501041,\"\"), (-1777376094,\"\"), (-2146599456,\"\"), (1638764184,\"\"), (2132630876,\"\"), (-558803310,\"\"), (-1704996955,\"\"), (1554848983,\"\"), (-643897612,\"\"), (277879516,\"\"), (1658643371,\"\"), (-239693190,\"\"), (-1096516904,\"\"), (1369629706,\"\"), (2053951917,\"\"), (1404211726,\"\"), (-496476476,\"\"), (66848233,\"\"), (-297738899,\"\"), (1003038978,\"\"), (229228928,\"\"), (-120662840,\"\"), (1068215916,\"\"), (-475916955,\"\"), (-443907787,\"\"), (2075776880,\"\"), (230583223,\"\"), (1330534066,\"\"), (-1120787669,\"\"), (-1584074738,\"\"), (-1702092513,\"\"), (1354781654,\"\"), (-1494390031,\"\"), (-310258093,\"\"), (1911875298,\"\")..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//k-gram\n",
    "val k = 5\n",
    "val k_gram1 = text1.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD1 = sc.parallelize(k_gram1)\n",
    "val filtered1 = kgramRDD1.distinct()\n",
    "\n",
    "val k_gram2 = text2.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD2 = sc.parallelize(k_gram2)\n",
    "val filtered2 = kgramRDD2.distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51371205\n",
      "0.48628795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join = MapPartitionsRDD[52] at leftOuterJoin at <console>:35\n",
       "join_n = 1049.0\n",
       "union = MapPartitionsRDD[56] at distinct at <console>:37\n",
       "union_n = 2042.0\n",
       "sim = 0.51371205\n",
       "jacc_dist = 0.48628795\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.48628795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val join = filtered1.leftOuterJoin(filtered2)\n",
    "val join_n = join.count().toFloat\n",
    "val union = filtered1.union(filtered2).distinct()\n",
    "val union_n = union.count().toFloat\n",
    "\n",
    "val sim = join_n / union_n\n",
    "val jacc_dist = 1-sim\n",
    "\n",
    "println(sim)\n",
    "println(jacc_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with Min-hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 2432.0 failed 1 times, most recent failure: Lost task 1.0 in stage 2432.0 (TID 6682, localhost, executor driver): java.lang.NullPointerException\n",
       "\tat $line8.$read$$iw$$iw$$iw$$iw.sc(<console>:17)\n",
       "\tat $line604.$read$$iw$$iw$$iw$$iw$$iw$$iw.parseFile(<console>:78)\n",
       "\tat $line604.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:81)\n",
       "\tat $line604.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:81)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat sc(<console>:17)\n",
       "\tat parseFile(<console>:78)\n",
       "\tat $anonfun$1.apply(<console>:81)\n",
       "\tat $anonfun$1.apply(<console>:81)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.NullPointerException\n",
       "  at sc(<console>:17)\n",
       "  at parseFile(<console>:78)\n",
       "  at $anonfun$1.apply(<console>:81)\n",
       "  at $anonfun$1.apply(<console>:81)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "val dir = new File(\"dataset_abstracts\")\n",
    "val files = dir.listFiles() //379\n",
    "val maxSize = 10\n",
    "val docSize = if(files.size < maxSize) files.size else maxSize\n",
    "\n",
    "def parseFile(f:java.io.File){\n",
    "    sc.textFile(\"./dataset_abstracts/\" + f.getName() ).map(line=>line.replaceAll(\"(\\\\s)+\", \" \")).reduce(_ + _)\n",
    "}\n",
    "\n",
    "val docs = sc.parallelize(files).map(parseFile)\n",
    "\n",
    "//val rows = sc.textFile(\"dataset_abstracts/a9000045.txt\").map(line=>line.replaceAll(\"(\\\\s)+\", \" \"))\n",
    "//val text = rows.reduce(_ + _).split(\"Abstract :\")(1)\n",
    "docs.collect().take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 6 in stage 2430.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2430.0 (TID 6671, localhost, executor driver): java.lang.NullPointerException\n",
       "\tat $line8.$read$$iw$$iw$$iw$$iw.sc(<console>:17)\n",
       "\tat $line564.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:75)\n",
       "\tat $line564.$read$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:75)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat sc(<console>:17)\n",
       "\tat $anonfun$1.apply(<console>:75)\n",
       "\tat $anonfun$1.apply(<console>:75)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.NullPointerException\n",
       "  at sc(<console>:17)\n",
       "  at $anonfun$1.apply(<console>:75)\n",
       "  at $anonfun$1.apply(<console>:75)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
       "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
       "  at scala.collection.AbstractIterator.to(Iterator.scala:1334)\n",
       "  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
       "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n",
       "  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
       "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.collect().take(1)\n",
    "//val abstracts = docs.map(doc => doc.split(\"Abstract :\")(1))\n",
    "//abstracts.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p = 1073676287\n",
       "m = 1073676288\n",
       "doc_signature = Array(-342689503, -841956040, -99892218, -932095214, -577548700, -585859158, -619577524, -929598556, -903111229, -549820996, -849171872, -1020008119, -775718799, -998815566, -860483878, -600152099, -608684411, -505264634, -1026611494, -460002885, -901909936, -819056087, -819216546, -864331178, -725024484, -454154236, -296771822, -957145494, -798275923, -574231678, -1037749267, -1009271244, -975740647, -682134308, -770975105, -679068068, -1021970304, -239090677, -775895460, -431311941, -497539779, -412289549, -804483231, -385354347, -788001867, -1036339276, -541172357, -1054658366, -917418843, -686658522, -431932535, -799241785, -784821473, -371596209, -1013243213, -435818998, -...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hashThis: (a: Long, b: Long, x: Long)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(-342689503, -841956040, -99892218, -932095214, -577548700, -585859158, -619577524, -929598556, -903111229, -549820996, -849171872, -1020008119, -775718799, -998815566, -860483878, -600152099, -608684411, -505264634, -1026611494, -460002885, -901909936, -819056087, -819216546, -864331178, -725024484, -454154236, -296771822, -957145494, -798275923, -574231678, -1037749267, -1009271244, -975740647, -682134308, -770975105, -679068068, -1021970304, -239090677, -775895460, -431311941, -497539779, -412289549, -804483231, -385354347, -788001867, -1036339276, -541172357, -1054658366, -917418843, -686658522, -431932535, -799241785, -784821473, -371596209, -1013243213, -435818998, -..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Min-hashing\n",
    "\n",
    "val fileName1 = \"dataset_abstracts/a9000006.txt\"\n",
    "val fileName2 = \"dataset_abstracts/a9000031.txt\"\n",
    "\n",
    "val rows1 = sc.textFile(fileName1).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text1 = rows1.reduce(_ + _)\n",
    "\n",
    "val rows2 = sc.textFile(fileName2).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text2 = rows2.reduce(_ + _)\n",
    "\n",
    "//k-gram\n",
    "val k = 5\n",
    "val k_gram1 = text1.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD1 = sc.parallelize(k_gram1)\n",
    "val filtered1 = kgramRDD1.distinct()\n",
    "\n",
    "\n",
    "\n",
    "val docSize = 10\n",
    "val docs = \n",
    "\n",
    "\n",
    "\n",
    "val p = 1073676287\n",
    "val m = p + 1\n",
    "def hashThis(a:Long, b:Long, x:Long): Long = {\n",
    "    ((a*x + b) % p ) % m\n",
    "}\n",
    "\n",
    "//TODO: Outer loop: foreach document in set of documents\n",
    "var doc_signature:Array[Long] = new Array[Long](100)\n",
    "for(i<-0 to 99){\n",
    "    \n",
    "    val r = new scala.util.Random(i)\n",
    "    val a2 = r.nextInt(p-1)\n",
    "    val a = if(a2%2==0) a2+1 else a2\n",
    "    val b = r.nextInt(p-1)\n",
    "    \n",
    "    for(d in doc){\n",
    "        val min = d.minhashThis()\n",
    "    }\n",
    "    \n",
    "    val min = filtered1.map(s => s._1.toLong).reduce((x,y) => hashThis(a, b, x) min hashThis(a, b, y) )\n",
    "    \n",
    "    doc_signature(i) = min\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
