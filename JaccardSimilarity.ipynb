{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileName1 = dataset_abstracts/a9000006.txt\n",
       "fileName2 = dataset_abstracts/a9000031.txt\n",
       "rows1 = MapPartitionsRDD[32] at map at <console>:36\n",
       "text1 = world-wide distribution, but the Atlantic and Pacificpopulations of the northern hemisphere appear to be discretepopulations, as is the population of the southern hemisphericoceans. Each of these oceanic populations may be furthersubdivided into smaller isolates, each with its own migratorypattern and somewhat distinct gene pool. This study willprovide information on the level of genetic isolation amongpopulations and the levels of gene flow and genealogicalrelationships among populations. This detailed geneticinformation will facilitate international policy decisionsregarding the conse...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "world-wide distribution, but the Atlantic and Pacificpopulations of the northern hemisphere appear to be discretepopulations, as is the population of the southern hemisphericoceans. Each of these oceanic populations may be furthersubdivided into smaller isolates, each with its own migratorypattern and somewhat distinct gene pool. This study willprovide information on the level of genetic isolation amongpopulations and the levels of gene flow and genealogicalrelationships among populations. This detailed geneticinformation will facilitate international policy decisionsregarding the conse..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//input\n",
    "val fileName1 = \"dataset_abstracts/a9000006.txt\"\n",
    "val fileName2 = \"dataset_abstracts/a9000031.txt\"\n",
    "\n",
    "val rows1 = sc.textFile(fileName).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text1 = rows1.reduce(_ + _)\n",
    "//val test = text.take(1)\n",
    "//println(test)\n",
    "\n",
    "val rows2 = sc.textFile(fileName2).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text2 = rows2.reduce(_ + _)\n",
    "//println(text2.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k = 5\n",
       "k_gram1 = List((-679493040,dataset_abstracts/a9000006.txt), (2019085497,dataset_abstracts/a9000006.txt), (-1450498693,dataset_abstracts/a9000006.txt), (569023187,dataset_abstracts/a9000006.txt), (997395134,dataset_abstracts/a9000006.txt), (-2085731237,dataset_abstracts/a9000006.txt), (-1231717628,dataset_abstracts/a9000006.txt), (-377010459,dataset_abstracts/a9000006.txt), (-1101467996,dataset_abstracts/a9000006.txt), (-572863609,dataset_abstracts/a9000006.txt), (-1452639189,dataset_abstracts/a9000006.txt), (1059127100,dataset_abstracts/a9000006.txt), (-387969419,dataset_abstracts/a9000006.txt), (-585145512,dataset_abstracts/a9000006.txt), (496721826,dataset_abstracts/a9000006.txt), (342454299,dataset_abstracts/a9000006.txt), (2015323395,dataset_abstracts...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((-679493040,dataset_abstracts/a9000006.txt), (2019085497,dataset_abstracts/a9000006.txt), (-1450498693,dataset_abstracts/a9000006.txt), (569023187,dataset_abstracts/a9000006.txt), (997395134,dataset_abstracts/a9000006.txt), (-2085731237,dataset_abstracts/a9000006.txt), (-1231717628,dataset_abstracts/a9000006.txt), (-377010459,dataset_abstracts/a9000006.txt), (-1101467996,dataset_abstracts/a9000006.txt), (-572863609,dataset_abstracts/a9000006.txt), (-1452639189,dataset_abstracts/a9000006.txt), (1059127100,dataset_abstracts/a9000006.txt), (-387969419,dataset_abstracts/a9000006.txt), (-585145512,dataset_abstracts/a9000006.txt), (496721826,dataset_abstracts/a9000006.txt), (342454299,dataset_abstracts/a9000006.txt), (2015323395,dataset_abstracts..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//k-gram\n",
    "val k = 5\n",
    "val k_gram1 = text1.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), fileName1))\n",
    "val kgramRDD1 = sc.parallelize(k_gram1)\n",
    "//val count = kgramRDD.count()\n",
    "val filtered1 = kgramRDD1.groupByKey()\n",
    "//val count2 = k_gram_filtered.count()\n",
    "//println(count)\n",
    "//println(count2)\n",
    "val k_gram2 = text2.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), fileName2))\n",
    "val kgramRDD2 = sc.parallelize(k_gram2)\n",
    "//val count = kgramRDD.count()\n",
    "val filtered2 = kgramRDD2.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07223989\n",
      "0.9277601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join = MapPartitionsRDD[58] at join at <console>:33\n",
       "join_n = 159.0\n",
       "union = PartitionerAwareUnionRDD[59] at union at <console>:35\n",
       "union_n = 2201.0\n",
       "sim = 0.07223989\n",
       "jacc_dist = 0.9277601\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9277601"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val join = filtered1.join(filtered2)\n",
    "val join_n = join.count().toFloat\n",
    "val union = filtered1.union(filtered2)\n",
    "val union_n = union.count().toFloat\n",
    "\n",
    "val sim = join_n / union_n\n",
    "val jacc_dist = 1-sim\n",
    "\n",
    "println(sim)\n",
    "println(jacc_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with Min-hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Min-hashing\n",
    "\n",
    "val data = Array()\n",
    "data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
