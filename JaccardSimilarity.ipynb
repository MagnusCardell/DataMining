{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the implementation of different algorithms related to finding textually similar documents. Based on Jaccard similarity, we implement shingling, minhashing, and locality-sensitive hashing (LHS). \n",
    "\n",
    "Using a collection of NSF research award abstracts obtained from https://archive.ics.uci.edu/ml/datasets/NSF+Research+Award+Abstracts+1990-2003 we produce a collection of vectors containing hashed shingles based on k-grams of size 5 hashed using MurmurHash. We then use minhash with 100 different hashing algorithms by randomly generating coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileName1 = dataset_abstracts/a9000038.txt\n",
       "fileName2 = dataset_abstracts/a9000040.txt\n",
       "rows1 = MapPartitionsRDD[2] at map at <console>:31\n",
       "text1 = principal investigator and associates. Topics in the followingareas are to be considered: (1) controlled Markov diffusionsand nonlinear PDEs; (2) asymptotic properties of nearlydeterministic Markov processes; (3) financial economicsapplications; (4) singular stochastic control; (5) computationalmethods in stochastic control; (6) stochastic calculus ofvariations; (7) nonlinear estimation. Analytical methods basedon viscosity solution techniques for nonlinear differentialequations as well as probabilistic methods will be studied.These theoretical studies are the basis for applied problemsr...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "principal investigator and associates. Topics in the followingareas are to be considered: (1) controlled Markov diffusionsand nonlinear PDEs; (2) asymptotic properties of nearlydeterministic Markov processes; (3) financial economicsapplications; (4) singular stochastic control; (5) computationalmethods in stochastic control; (6) stochastic calculus ofvariations; (7) nonlinear estimation. Analytical methods basedon viscosity solution techniques for nonlinear differentialequations as well as probabilistic methods will be studied.These theoretical studies are the basis for applied problemsr..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//input\n",
    "val fileName1 = \"dataset_abstracts/a9000038.txt\"\n",
    "val fileName2 = \"dataset_abstracts/a9000040.txt\"\n",
    "\n",
    "val rows1 = sc.textFile(fileName1).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text1 = rows1.reduce(_ + _)\n",
    "\n",
    "val rows2 = sc.textFile(fileName2).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text2 = rows2.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k = 5\n",
       "k_gram1 = List((323183978,\"\"), (879426833,\"\"), (207652440,\"\"), (1023262041,\"\"), (1191319030,\"\"), (-1029485329,\"\"), (94755376,\"\"), (798376504,\"\"), (-1163905680,\"\"), (982395043,\"\"), (-487295416,\"\"), (929914478,\"\"), (1998737565,\"\"), (2078865377,\"\"), (-1593058553,\"\"), (-81630221,\"\"), (1675405219,\"\"), (-1975782702,\"\"), (-312516106,\"\"), (-2146848343,\"\"), (899011444,\"\"), (-2008099792,\"\"), (1597990745,\"\"), (1159842749,\"\"), (2046055236,\"\"), (-1557845639,\"\"), (-2100210802,\"\"), (-206169076,\"\"), (-19109906,\"\"), (1127829784,\"\"), (-121854170,\"\"), (772595528,\"\"), (734667765,\"\"), (-1813676078,\"\"), (-708552550,\"\"), (-1949504179,\"\"), (-707004830,\"\"), (179624194,\"\"), (-1471358333,\"\"), (45318084,\"\"), (2014423945,\"\"), (-618699956,\"\"), (1272547154,\"\"), (240336297,\"\"), (9266125...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((323183978,\"\"), (879426833,\"\"), (207652440,\"\"), (1023262041,\"\"), (1191319030,\"\"), (-1029485329,\"\"), (94755376,\"\"), (798376504,\"\"), (-1163905680,\"\"), (982395043,\"\"), (-487295416,\"\"), (929914478,\"\"), (1998737565,\"\"), (2078865377,\"\"), (-1593058553,\"\"), (-81630221,\"\"), (1675405219,\"\"), (-1975782702,\"\"), (-312516106,\"\"), (-2146848343,\"\"), (899011444,\"\"), (-2008099792,\"\"), (1597990745,\"\"), (1159842749,\"\"), (2046055236,\"\"), (-1557845639,\"\"), (-2100210802,\"\"), (-206169076,\"\"), (-19109906,\"\"), (1127829784,\"\"), (-121854170,\"\"), (772595528,\"\"), (734667765,\"\"), (-1813676078,\"\"), (-708552550,\"\"), (-1949504179,\"\"), (-707004830,\"\"), (179624194,\"\"), (-1471358333,\"\"), (45318084,\"\"), (2014423945,\"\"), (-618699956,\"\"), (1272547154,\"\"), (240336297,\"\"), (9266125..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//k-gram\n",
    "val k = 5\n",
    "val k_gram1 = text1.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD1 = sc.parallelize(k_gram1)\n",
    "val filtered1 = kgramRDD1.distinct()\n",
    "\n",
    "val k_gram2 = text2.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD2 = sc.parallelize(k_gram2)\n",
    "val filtered2 = kgramRDD2.distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6188993\n",
      "0.3811007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join = MapPartitionsRDD[16] at leftOuterJoin at <console>:28\n",
       "join_n = 1192.0\n",
       "union = MapPartitionsRDD[20] at distinct at <console>:30\n",
       "union_n = 1926.0\n",
       "sim = 0.6188993\n",
       "jacc_dist = 0.3811007\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3811007"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val join = filtered1.leftOuterJoin(filtered2)\n",
    "val join_n = join.count().toFloat\n",
    "val union = filtered1.union(filtered2).distinct()\n",
    "val union_n = union.count().toFloat\n",
    "\n",
    "val sim = join_n / union_n\n",
    "val jacc_dist = 1-sim\n",
    "\n",
    "println(sim)\n",
    "println(jacc_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with Min-hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a9000038.txt,Title : Mathematical Sciences: Research on Optimal Stochastic Control and Nonlinear EstimationType : AwardNSF Org : DMS LatestAmendmentDate : April 8, 1992 File : a9000038Award Number: 9000038Award Instr.: Continuing grant Prgm Manager:  DMS DIVISION OF MATHEMATICAL SCIENCES  MPS DIRECT FOR MATHEMATICAL & PHYSICAL SCIENStart Date : July 1, 1990 Expires : December 31, 1993 (Estimated)ExpectedTotal Amt. : $188574 (Estimated)Investigator: Wendell H. Fleming whf@cfm.brown.edu (Principal Investigator current)Sponsor : Brown University 164 Angell Street Providence, RI 02912 401/863-2777NSF Program : 1266 APPLIED MATHEMATICSFld Applictn: 0000099 Other Applications NEC  21 Mathematics Program Ref : Abstract : This research is part of an on-going program by the  principal investigator and associates. Topics in the following  areas are to be considered: (1) controlled Markov diffusions  and nonlinear PDEs; (2) asymptotic properties of nearly  deterministic Markov processes; (3) financial economics  applications; (4) singular stochastic control; (5) computational  methods in stochastic control; (6) stochastic calculus of  variations; (7) nonlinear estimation. Analytical methods based  on viscosity solution techniques for nonlinear differential  equations as well as probabilistic methods will be studied.  These theoretical studies are the basis for applied problems  ranging from decisions at the stock market level to the control  of spaceships.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dir = dataset_abstracts\n",
       "files = Array(dataset_abstracts/a9000038.txt, dataset_abstracts/a9000040.txt, dataset_abstracts/a9000043.txt, dataset_abstracts/a9000045.txt, dataset_abstracts/a9000046.txt, dataset_abstracts/a9000048.txt, dataset_abstracts/a9000049.txt, dataset_abstracts/a9000050.txt, dataset_abstracts/a9000052.txt, dataset_abstracts/a9000053.txt, dataset_abstracts/a9000054.txt, dataset_abstracts/a9000057.txt, dataset_abstracts/a9000058.txt, dataset_abstracts/a9000060.txt, dataset_abstracts/a9000063.txt, dataset_abstracts/a9000075.txt, dataset_abstracts/a9000089.txt, dataset_abstracts/a9000091.txt, dataset_abstracts/a9000094.txt, dataset_abstracts/a9000099.txt, dataset_abstracts/a9000100.txt, dataset_abstracts/a9000102.txt, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(dataset_abstracts/a9000038.txt, dataset_abstracts/a9000040.txt, dataset_abstracts/a9000043.txt, dataset_abstracts/a9000045.txt, dataset_abstracts/a9000046.txt, dataset_abstracts/a9000048.txt, dataset_abstracts/a9000049.txt, dataset_abstracts/a9000050.txt, dataset_abstracts/a9000052.txt, dataset_abstracts/a9000053.txt, dataset_abstracts/a9000054.txt, dataset_abstracts/a9000057.txt, dataset_abstracts/a9000058.txt, dataset_abstracts/a9000060.txt, dataset_abstracts/a9000063.txt, dataset_abstracts/a9000075.txt, dataset_abstracts/a9000089.txt, dataset_abstracts/a9000091.txt, dataset_abstracts/a9000094.txt, dataset_abstracts/a9000099.txt, dataset_abstracts/a9000100.txt, dataset_abstracts/a9000102.txt, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "val dir = new File(\"dataset_abstracts\")\n",
    "val files = dir.listFiles() //379\n",
    "val maxSize = 10\n",
    "val docSize = if(files.size < maxSize) files.size else maxSize\n",
    "\n",
    "val docs = files.map(f => (f.getName(), sc.textFile(f.getPath() ).map(line=>line.replaceAll(\"(\\\\s)+\", \" \")).collect().reduce(_ + _)))\n",
    "val docsRDD = sc.parallelize(docs).cache()\n",
    "println(docsRDD.take(1)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[1156] at map at <console>:34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "abstracts = MapPartitionsRDD[1155] at map at <console>:30\n",
       "k = 5\n",
       "k_gram = MapPartitionsRDD[1156] at map at <console>:34\n",
       "documents = MapPartitionsRDD[1157] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1157] at map at <console>:36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val abstracts = docsRDD.map(doc => (doc._1,doc._2.split(\"Abstract :\"))).filter(a => a._2.size > 1).map(a => (a._1, a._2(1)))\n",
    "\n",
    "val k = 5\n",
    "val k_gram = abstracts.map(a => (a._1, a._2.split(\"\").sliding(k).toList.map(x => scala.util.hashing.MurmurHash3.arrayHash(x))))\n",
    "println(k_gram)\n",
    "val documents = k_gram.map(k => (k._1, k._2.distinct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p = 1073676287\n",
       "m = 1073676288\n",
       "n = 100\n",
       "defined class HashAlgo\n",
       "defined class DocHash\n",
       "defined class DocMinHash\n",
       "hash_algos = Array(HashAlgo(1,495872699,495872699), HashAlgo(2,496449823,496449822), HashAlgo(3,496257449,496257448), HashAlgo(4,495295577,495295576), HashAlgo(5,495103201,495103201), HashAlgo(6,495680325,495680325), HashAlgo(7,495487951,495487950), HashAlgo(8,494526079,494526078), HashAlgo(9,494333703,494333703), HashAlgo(10,494910827,494910827), HashAlgo(11,494718453,494718452), HashAlgo(12,493756581,493756580), HashAlgo(13,493564207,493564206), HashAlgo(14,494141329,494141329), HashAlgo(15,49394...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hashThis: (a: Long, b: Long, x: Long)Long\n",
       "generateCo1: (i: Int)Long\n",
       "generateCo2: (i: Int)Long\n",
       "findMin: (x: Long, y: Long, h: HashAlgo)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(HashAlgo(1,495872699,495872699), HashAlgo(2,496449823,496449822), HashAlgo(3,496257449,496257448), HashAlgo(4,495295577,495295576), HashAlgo(5,495103201,495103201), HashAlgo(6,495680325,495680325), HashAlgo(7,495487951,495487950), HashAlgo(8,494526079,494526078), HashAlgo(9,494333703,494333703), HashAlgo(10,494910827,494910827), HashAlgo(11,494718453,494718452), HashAlgo(12,493756581,493756580), HashAlgo(13,493564207,493564206), HashAlgo(14,494141329,494141329), HashAlgo(15,49394..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Min-hashing\n",
    "import Array._\n",
    "val p = 1073676287\n",
    "val m = p + 1\n",
    "def hashThis(a:Long, b:Long, x:Long): Long = {\n",
    "    ((a*x + b) % p ) % m\n",
    "}\n",
    "def generateCo1(i:Int): Long = {\n",
    "    val r = new scala.util.Random(i)\n",
    "    val a2 = r.nextInt(p-1)\n",
    "    val a = if(a2%2==0) a2+1 else a2\n",
    "    a\n",
    "}\n",
    "def generateCo2(i:Int): Long = {\n",
    "    val r = new scala.util.Random(i)\n",
    "    val b = r.nextInt(p-1)\n",
    "    b\n",
    "}\n",
    "\n",
    "def findMin(x:Long,y:Long, h:HashAlgo): Long = {\n",
    "    return hashThis(h.co1, h.co2, x) min hashThis(h.co1, h.co2, y)\n",
    "}\n",
    "val n = 100\n",
    "case class HashAlgo(n_th: Int, co1: Long, co2: Long)\n",
    "case class DocHash(hash: HashAlgo, n_th: Int, docId: String, shingles: List[Int] )\n",
    "case class DocMinHash(docId: String, minHash: Long, hashN: Int)\n",
    "\n",
    "\n",
    "val hash_algos = range(1,n+1).map(i => HashAlgo(i, generateCo1(i), generateCo2(i) ))\n",
    "//hash_algos.foreach(println)\n",
    "val prep_docs = documents.map(d => hash_algos.map(h => DocHash(h, h.n_th, d._1, d._2))).flatMap(y => y)\n",
    "//prep_docs.collect().foreach(println)\n",
    "val doc_signatures = prep_docs.map(d => DocMinHash(d.docId, d.shingles.map(s => s.toLong).reduce((a,b)=> findMin(a,b,d.hash)), d.n_th ) )\n",
    "//doc_signatures.collect().foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have a document collection like (docID, minhash, hash_n):\n",
    "\n",
    "DocMinHash(a9000038.txt,-970556178,1)\n",
    "DocMinHash(a9000040.txt,-319735484,1)\n",
    "DocMinHash(a9000043.txt,-921168316,1)\n",
    "DocMinHash(a9000045.txt,-691100492,1)\n",
    "DocMinHash(a9000046.txt,-295120213,1)\n",
    "DocMinHash(a9000048.txt,-527021014,1)\n",
    "DocMinHash(a9000049.txt,-295120213,1)\n",
    "DocMinHash(a9000050.txt,-911179693,1)\n",
    "DocMinHash(a9000052.txt,-474449840,1)\n",
    "DocMinHash(a9000053.txt,-788660299,1)\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "So we can map the values by hash value, get the common documents and return them as candidate pairs. For this, we can use DataFrame which has better aggregation functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+------------+-----------+----------+\n",
      "|       d1_id|    d1_hash|d1_minhash|       d2_id|    d2_hash|d2_minhash|\n",
      "+------------+-----------+----------+------------+-----------+----------+\n",
      "|a9000138.txt|-1010748709|         7|a9000139.txt|-1010748709|         7|\n",
      "|a9000138.txt| -937681875|        42|a9000139.txt| -937681875|        42|\n",
      "|a9000502.txt| -873889065|        99|a9000697.txt| -873889065|        99|\n",
      "|a9000650.txt| -857341462|        59|a9000826.txt| -857341462|        59|\n",
      "|a9000287.txt| -800533594|        55|a9000713.txt| -800533594|        55|\n",
      "|a9000287.txt| -800533594|        55|a9000816.txt| -800533594|        55|\n",
      "|a9000287.txt| -800533594|        55|a9000825.txt| -800533594|        55|\n",
      "|a9000713.txt| -800533594|        55|a9000816.txt| -800533594|        55|\n",
      "|a9000713.txt| -800533594|        55|a9000825.txt| -800533594|        55|\n",
      "|a9000816.txt| -800533594|        55|a9000825.txt| -800533594|        55|\n",
      "|a9000187.txt| -763014720|        38|a9000383.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000391.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000501.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000549.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000626.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000663.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000678.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000689.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000692.txt| -763014720|        38|\n",
      "|a9000187.txt| -763014720|        38|a9000765.txt| -763014720|        38|\n",
      "+------------+-----------+----------+------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "docDF = [docId: string, minHash: bigint ... 1 more field]\n",
       "sqlContext = org.apache.spark.sql.SQLContext@39a54505\n",
       "dup = [d1_id: string, d1_hash: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[d1_id: string, d1_hash: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "import org.apache.spark.sql.functions.collect_list\n",
    "import org.apache.spark.sql.functions.count\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val docDF = spark.createDataFrame(doc_signatures)\n",
    "//docDF.show(200)\n",
    "//docDF.printSchema\n",
    "//val group = docDF.groupBy(\"hashN\").agg(collect_set(\"docId\").as(\"newGroupedListColumn\"), collect_set(\"minHash\")).show()\n",
    "//group.show()\n",
    "\n",
    "docDF.createOrReplaceTempView(\"docs\")\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val dup = sqlContext.sql(\"SELECT d.docId as d1_id, d.minHash as d1_hash, d.hashN as d1_minhash, d2.docId as d2_id, d2.minHash as d2_hash, d2.hashN as d2_minhash FROM docs d, docs d2 WHERE d.minHash = d2.minHash AND d.hashN = d2.hashN AND d.docId != d2.docId AND d.docId < d2.docId\")\n",
    "dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,a9000049.txta9000251.txt)\n",
      "(1.0,a9000046.txta9000251.txt)\n",
      "(1.0,a9000223.txta9000528.txt)\n",
      "(1.0,a9000221.txta9000528.txt)\n",
      "(1.0,a9000246.txta9000463.txt)\n",
      "(1.0,a9000404.txta9000944.txt)\n",
      "(1.0,a9000158.txta9000246.txt)\n",
      "(1.0,a9000528.txta9000944.txt)\n",
      "(1.0,a9000221.txta9000404.txt)\n",
      "(1.0,a9000222.txta9000404.txt)\n",
      "(1.0,a9000343.txta9000463.txt)\n",
      "(1.0,a9000356.txta9000527.txt)\n",
      "(1.0,a9000049.txta9000246.txt)\n",
      "(1.0,a9000396.txta9000528.txt)\n",
      "(1.0,a9000223.txta9000944.txt)\n",
      "(1.0,a9000046.txta9000246.txt)\n",
      "(1.0,a9000049.txta9000158.txt)\n",
      "(1.0,a9000221.txta9000944.txt)\n",
      "(1.0,a9000223.txta9000396.txt)\n",
      "(1.0,a9000221.txta9000223.txt)\n",
      "(1.0,a9000046.txta9000158.txt)\n",
      "(1.0,a9000177.txta9000458.txt)\n",
      "(1.0,a9000057.txta9000915.txt)\n",
      "(1.0,a9000396.txta9000944.txt)\n",
      "(1.0,a9000222.txta9000528.txt)\n",
      "(1.0,a9000158.txta9000463.txt)\n",
      "(1.0,a9000246.txta9000343.txt)\n",
      "(1.0,a9000221.txta9000222.txt)\n",
      "(1.0,a9000223.txta9000404.txt)\n",
      "(1.0,a9000158.txta9000343.txt)\n",
      "(1.0,a9000049.txta9000463.txt)\n",
      "(1.0,a9000046.txta9000049.txt)\n",
      "(1.0,a9000246.txta9000251.txt)\n",
      "(1.0,a9000046.txta9000463.txt)\n",
      "(1.0,a9000222.txta9000944.txt)\n",
      "(1.0,a9000222.txta9000223.txt)\n",
      "(1.0,a9000409.txta9000410.txt)\n",
      "(1.0,a9000221.txta9000396.txt)\n",
      "(1.0,a9000222.txta9000396.txt)\n",
      "(1.0,a9000049.txta9000343.txt)\n",
      "(1.0,a9000251.txta9000463.txt)\n",
      "(1.0,a9000158.txta9000251.txt)\n",
      "(1.0,a9000404.txta9000528.txt)\n",
      "(1.0,a9000046.txta9000343.txt)\n",
      "(1.0,a9000396.txta9000404.txt)\n",
      "(1.0,a9000251.txta9000343.txt)\n",
      "(0.96,a9000383.txta9000663.txt)\n",
      "(0.96,a9000343.txta9000393.txt)\n",
      "(0.96,a9000246.txta9000393.txt)\n",
      "(0.96,a9000158.txta9000393.txt)\n",
      "(0.96,a9000741.txta9000884.txt)\n",
      "(0.96,a9000049.txta9000393.txt)\n",
      "(0.96,a9000046.txta9000393.txt)\n",
      "(0.96,a9000251.txta9000393.txt)\n",
      "(0.96,a9000393.txta9000463.txt)\n",
      "(0.91,a9000814.txta9000829.txt)\n",
      "(0.91,a9000365.txta9000598.txt)\n",
      "(0.81,a9000502.txta9000697.txt)\n",
      "(0.76,a9000121.txta9000593.txt)\n",
      "(0.75,a9000527.txta9000962.txt)\n",
      "(0.75,a9000356.txta9000962.txt)\n",
      "(0.75,a9000638.txta9000742.txt)\n",
      "(0.74,a9000574.txta9000810.txt)\n",
      "(0.73,a9000638.txta9000725.txt)\n",
      "(0.73,a9000654.txta9000798.txt)\n",
      "(0.72,a9000138.txta9000139.txt)\n",
      "(0.71,a9000489.txta9000979.txt)\n",
      "(0.7,a9000725.txta9000742.txt)\n",
      "(0.66,a9000134.txta9000720.txt)\n",
      "(0.65,a9000365.txta9000915.txt)\n",
      "(0.65,a9000057.txta9000365.txt)\n",
      "(0.63,a9000598.txta9000915.txt)\n",
      "(0.63,a9000057.txta9000598.txt)\n",
      "(0.6,a9000139.txta9000845.txt)\n",
      "(0.6,a9000619.txta9000798.txt)\n",
      "(0.59,a9000802.txta9000814.txt)\n",
      "(0.59,a9000138.txta9000845.txt)\n",
      "(0.59,a9000802.txta9000829.txt)\n",
      "(0.58,a9000367.txta9000893.txt)\n",
      "(0.56,a9000127.txta9000641.txt)\n",
      "(0.56,a9000731.txta9000815.txt)\n",
      "(0.54,a9000466.txta9000794.txt)\n",
      "(0.53,a9000712.txta9000806.txt)\n",
      "(0.53,a9000729.txta9000748.txt)\n",
      "(0.52,a9000765.txta9000864.txt)\n",
      "(0.52,a9000450.txta9000943.txt)\n",
      "(0.52,a9000459.txta9000729.txt)\n",
      "(0.52,a9000795.txta9000943.txt)\n",
      "(0.51,a9000619.txta9000654.txt)\n",
      "(0.5,a9000450.txta9000590.txt)\n",
      "(0.5,a9000590.txta9000943.txt)\n",
      "(0.49,a9000590.txta9000795.txt)\n",
      "(0.47,a9000450.txta9000795.txt)\n",
      "(0.45,a9000459.txta9000748.txt)\n",
      "(0.45,a9000671.txta9000713.txt)\n",
      "(0.44,a9000676.txta9000713.txt)\n",
      "(0.44,a9000671.txta9000676.txt)\n",
      "(0.43,a9000446.txta9000597.txt)\n",
      "(0.43,a9000287.txta9000713.txt)\n",
      "(0.42,a9000671.txta9000816.txt)\n",
      "(0.42,a9000287.txta9000816.txt)\n",
      "(0.42,a9000391.txta9000772.txt)\n",
      "(0.42,a9000713.txta9000816.txt)\n",
      "(0.41,a9000287.txta9000671.txt)\n",
      "(0.41,a9000326.txta9000597.txt)\n",
      "(0.41,a9000117.txta9000460.txt)\n",
      "(0.4,a9000671.txta9000825.txt)\n",
      "(0.4,a9000287.txta9000676.txt)\n",
      "(0.4,a9000713.txta9000825.txt)\n",
      "(0.4,a9000446.txta9000560.txt)\n",
      "(0.4,a9000182.txta9000641.txt)\n",
      "(0.4,a9000501.txta9000772.txt)\n",
      "(0.4,a9000676.txta9000816.txt)\n",
      "(0.4,a9000501.txta9000765.txt)\n",
      "(0.4,a9000326.txta9000446.txt)\n",
      "(0.4,a9000326.txta9000450.txt)\n",
      "(0.4,a9000772.txta9000864.txt)\n",
      "(0.4,a9000689.txta9000772.txt)\n",
      "(0.39,a9000501.txta9000864.txt)\n",
      "(0.39,a9000676.txta9000825.txt)\n",
      "(0.38,a9000501.txta9000689.txt)\n",
      "(0.38,a9000406.txta9000892.txt)\n",
      "(0.38,a9000117.txta9000254.txt)\n",
      "(0.38,a9000326.txta9000943.txt)\n",
      "(0.38,a9000197.txta9000206.txt)\n",
      "(0.38,a9000626.txta9000772.txt)\n",
      "(0.38,a9000765.txta9000772.txt)\n",
      "(0.38,a9000501.txta9000626.txt)\n",
      "(0.38,a9000383.txta9000501.txt)\n",
      "(0.38,a9000326.txta9000560.txt)\n",
      "(0.38,a9000501.txta9000663.txt)\n",
      "(0.37,a9000626.txta9000663.txt)\n",
      "(0.37,a9000663.txta9000765.txt)\n",
      "(0.37,a9000326.txta9000590.txt)\n",
      "(0.37,a9000369.txta9000753.txt)\n",
      "(0.37,a9000626.txta9000689.txt)\n",
      "(0.37,a9000127.txta9000182.txt)\n",
      "(0.37,a9000678.txta9000772.txt)\n",
      "(0.37,a9000392.txta9000466.txt)\n",
      "(0.37,a9000187.txta9000772.txt)\n",
      "(0.37,a9000195.txta9000460.txt)\n",
      "(0.37,a9000780.txta9000903.txt)\n",
      "(0.37,a9000689.txta9000864.txt)\n",
      "(0.37,a9000287.txta9000825.txt)\n",
      "(0.37,a9000626.txta9000864.txt)\n",
      "(0.37,a9000406.txta9000741.txt)\n",
      "(0.37,a9000117.txta9000195.txt)\n",
      "(0.37,a9000175.txta9000373.txt)\n",
      "(0.37,a9000391.txta9000501.txt)\n",
      "(0.37,a9000383.txta9000765.txt)\n",
      "(0.37,a9000626.txta9000765.txt)\n",
      "(0.37,a9000692.txta9000772.txt)\n",
      "(0.37,a9000383.txta9000626.txt)\n",
      "(0.37,a9000436.txta9000687.txt)\n",
      "(0.36,a9000663.txta9000772.txt)\n",
      "(0.36,a9000392.txta9000794.txt)\n",
      "(0.36,a9000816.txta9000825.txt)\n",
      "(0.36,a9000446.txta9000795.txt)\n",
      "(0.36,a9000741.txta9000892.txt)\n",
      "(0.36,a9000195.txta9000254.txt)\n",
      "(0.36,a9000829.txta9000877.txt)\n",
      "(0.36,a9000531.txta9000874.txt)\n",
      "(0.36,a9000815.txta9000830.txt)\n",
      "(0.36,a9000326.txta9000795.txt)\n",
      "(0.36,a9000383.txta9000772.txt)\n",
      "(0.36,a9000406.txta9000884.txt)\n",
      "(0.36,a9000112.txta9000201.txt)\n",
      "(0.36,a9000446.txta9000943.txt)\n",
      "(0.36,a9000740.txta9000830.txt)\n",
      "(0.35,a9000689.txta9000765.txt)\n",
      "(0.35,a9000254.txta9000460.txt)\n",
      "(0.35,a9000560.txta9000597.txt)\n",
      "(0.35,a9000383.txta9000689.txt)\n",
      "(0.35,a9000295.txta9000326.txt)\n",
      "(0.35,a9000312.txta9000560.txt)\n",
      "(0.35,a9000884.txta9000892.txt)\n",
      "(0.35,a9000446.txta9000450.txt)\n",
      "(0.35,a9000383.txta9000391.txt)\n",
      "(0.35,a9000094.txta9000373.txt)\n",
      "(0.35,a9000383.txta9000864.txt)\n",
      "(0.35,a9000257.txta9000469.txt)\n",
      "(0.35,a9000186.txta9000779.txt)\n",
      "(0.35,a9000549.txta9000859.txt)\n",
      "(0.35,a9000391.txta9000678.txt)\n",
      "(0.35,a9000391.txta9000626.txt)\n",
      "(0.35,a9000689.txta9000859.txt)\n",
      "(0.35,a9000814.txta9000877.txt)\n",
      "(0.35,a9000663.txta9000689.txt)\n",
      "(0.35,a9000644.txta9000753.txt)\n",
      "(0.35,a9000643.txta9000874.txt)\n",
      "(0.35,a9000597.txta9000943.txt)\n",
      "(0.35,a9000501.txta9000678.txt)\n",
      "(0.35,a9000391.txta9000663.txt)\n",
      "(0.35,a9000597.txta9000795.txt)\n",
      "(0.35,a9000663.txta9000864.txt)\n",
      "(0.35,a9000312.txta9000446.txt)\n",
      "(0.34,a9000187.txta9000391.txt)\n",
      "(0.34,a9000295.txta9000446.txt)\n",
      "(0.34,a9000560.txta9000943.txt)\n",
      "(0.34,a9000549.txta9000678.txt)\n",
      "(0.34,a9000446.txta9000465.txt)\n",
      "(0.34,a9000187.txta9000864.txt)\n",
      "(0.34,a9000391.txta9000864.txt)\n",
      "(0.34,a9000549.txta9000864.txt)\n",
      "(0.34,a9000450.txta9000597.txt)\n",
      "(0.34,a9000678.txta9000765.txt)\n",
      "(0.34,a9000187.txta9000765.txt)\n",
      "(0.34,a9000295.txta9000560.txt)\n",
      "(0.34,a9000391.txta9000765.txt)\n",
      "(0.34,a9000549.txta9000772.txt)\n",
      "(0.34,a9000526.txta9000595.txt)\n",
      "(0.34,a9000326.txta9000465.txt)\n",
      "(0.34,a9000731.txta9000740.txt)\n",
      "(0.34,a9000369.txta9000644.txt)\n",
      "(0.34,a9000678.txta9000864.txt)\n",
      "(0.34,a9000451.txta9000706.txt)\n",
      "(0.34,a9000450.txta9000560.txt)\n",
      "(0.34,a9000626.txta9000678.txt)\n",
      "(0.34,a9000650.txta9000826.txt)\n",
      "(0.34,a9000458.txta9000892.txt)\n",
      "(0.34,a9000391.txta9000689.txt)\n",
      "(0.34,a9000859.txta9000864.txt)\n",
      "(0.34,a9000326.txta9000666.txt)\n",
      "(0.34,a9000177.txta9000892.txt)\n",
      "(0.34,a9000295.txta9000465.txt)\n",
      "(0.33,a9000689.txta9000692.txt)\n",
      "(0.33,a9000772.txta9000859.txt)\n",
      "(0.33,a9000187.txta9000501.txt)\n",
      "(0.33,a9000040.txta9000369.txt)\n",
      "(0.33,a9000663.txta9000692.txt)\n",
      "(0.33,a9000257.txta9000292.txt)\n",
      "(0.33,a9000663.txta9000859.txt)\n",
      "(0.33,a9000731.txta9000830.txt)\n",
      "(0.33,a9000295.txta9000450.txt)\n",
      "(0.33,a9000312.txta9000666.txt)\n",
      "(0.33,a9000590.txta9000597.txt)\n",
      "(0.33,a9000465.txta9000943.txt)\n",
      "(0.33,a9000292.txta9000469.txt)\n",
      "(0.33,a9000177.txta9000406.txt)\n",
      "(0.33,a9000549.txta9000765.txt)\n",
      "(0.33,a9000549.txta9000689.txt)\n",
      "(0.33,a9000465.txta9000560.txt)\n",
      "(0.33,a9000501.txta9000859.txt)\n",
      "(0.33,a9000383.txta9000692.txt)\n",
      "(0.33,a9000501.txta9000549.txt)\n",
      "(0.33,a9000383.txta9000859.txt)\n",
      "(0.33,a9000626.txta9000859.txt)\n",
      "(0.33,a9000406.txta9000458.txt)\n",
      "(0.33,a9000560.txta9000795.txt)\n",
      "(0.33,a9000446.txta9000590.txt)\n",
      "(0.33,a9000295.txta9000943.txt)\n",
      "(0.33,a9000465.txta9000597.txt)\n",
      "(0.33,a9000312.txta9000326.txt)\n",
      "(0.32,a9000369.txta9000636.txt)\n",
      "(0.32,a9000549.txta9000626.txt)\n",
      "(0.32,a9000040.txta9000644.txt)\n",
      "(0.32,a9000802.txta9000877.txt)\n",
      "(0.32,a9000740.txta9000815.txt)\n",
      "(0.32,a9000391.txta9000859.txt)\n",
      "(0.32,a9000187.txta9000678.txt)\n",
      "(0.32,a9000312.txta9000943.txt)\n",
      "(0.32,a9000501.txta9000692.txt)\n",
      "(0.32,a9000531.txta9000643.txt)\n",
      "(0.32,a9000312.txta9000597.txt)\n",
      "(0.32,a9000626.txta9000692.txt)\n",
      "(0.32,a9000094.txta9000175.txt)\n",
      "(0.32,a9000692.txta9000765.txt)\n",
      "(0.32,a9000678.txta9000689.txt)\n",
      "(0.32,a9000187.txta9000689.txt)\n",
      "(0.32,a9000295.txta9000597.txt)\n",
      "(0.32,a9000040.txta9000753.txt)\n",
      "(0.32,a9000378.txta9000390.txt)\n",
      "(0.31,a9000446.txta9000666.txt)\n",
      "(0.31,a9000040.txta9000636.txt)\n",
      "(0.31,a9000663.txta9000678.txt)\n",
      "(0.31,a9000549.txta9000663.txt)\n",
      "(0.31,a9000450.txta9000465.txt)\n",
      "(0.31,a9000187.txta9000626.txt)\n",
      "(0.31,a9000465.txta9000795.txt)\n",
      "(0.31,a9000383.txta9000549.txt)\n",
      "(0.31,a9000734.txta9000984.txt)\n",
      "(0.31,a9000383.txta9000678.txt)\n",
      "(0.31,a9000636.txta9000644.txt)\n",
      "(0.31,a9000678.txta9000859.txt)\n",
      "(0.31,a9000692.txta9000859.txt)\n",
      "(0.31,a9000560.txta9000590.txt)\n",
      "(0.3,a9000765.txta9000859.txt)\n",
      "(0.3,a9000653.txta9000797.txt)\n",
      "(0.3,a9000187.txta9000383.txt)\n",
      "(0.3,a9000295.txta9000590.txt)\n",
      "(0.3,a9000560.txta9000666.txt)\n",
      "(0.3,a9000187.txta9000663.txt)\n",
      "(0.3,a9000465.txta9000590.txt)\n",
      "(0.3,a9000295.txta9000666.txt)\n",
      "(0.3,a9000549.txta9000692.txt)\n",
      "(0.3,a9000295.txta9000795.txt)\n",
      "(0.3,a9000295.txta9000312.txt)\n",
      "(0.29,a9000312.txta9000450.txt)\n",
      "(0.29,a9000458.txta9000741.txt)\n",
      "(0.29,a9000177.txta9000741.txt)\n",
      "(0.29,a9000216.txta9000273.txt)\n",
      "(0.29,a9000391.txta9000692.txt)\n",
      "(0.29,a9000391.txta9000549.txt)\n",
      "(0.29,a9000465.txta9000666.txt)\n",
      "(0.29,a9000692.txta9000864.txt)\n",
      "(0.29,a9000271.txta9000615.txt)\n",
      "(0.29,a9000597.txta9000666.txt)\n",
      "(0.29,a9000636.txta9000753.txt)\n",
      "(0.28,a9000458.txta9000884.txt)\n",
      "(0.28,a9000187.txta9000859.txt)\n",
      "(0.28,a9000177.txta9000884.txt)\n",
      "(0.28,a9000312.txta9000590.txt)\n",
      "(0.28,a9000312.txta9000465.txt)\n",
      "(0.28,a9000187.txta9000549.txt)\n",
      "(0.27,a9000666.txta9000943.txt)\n",
      "(0.27,a9000582.txta9000937.txt)\n",
      "(0.27,a9000312.txta9000795.txt)\n",
      "(0.27,a9000678.txta9000692.txt)\n",
      "(0.27,a9000590.txta9000666.txt)\n",
      "(0.26,a9000187.txta9000692.txt)\n",
      "(0.26,a9000450.txta9000666.txt)\n",
      "(0.25,a9000666.txta9000795.txt)\n",
      "(0.25,a9000060.txta9000146.txt)\n",
      "(0.2,a9000436.txta9000757.txt)\n",
      "(0.17,a9000687.txta9000757.txt)\n",
      "(0.11,a9000325.txta9000442.txt)\n",
      "(0.1,a9000560.txta9000765.txt)\n",
      "(0.09,a9000560.txta9000864.txt)\n",
      "(0.03,a9000833.txta9000844.txt)\n",
      "(0.02,a9000050.txta9000327.txt)\n",
      "(0.02,a9000466.txta9000770.txt)\n",
      "(0.01,a9000171.txta9000942.txt)\n",
      "(0.01,a9000075.txta9000356.txt)\n",
      "(0.01,a9000075.txta9000527.txt)\n",
      "(0.01,a9000745.txta9000879.txt)\n",
      "(0.01,a9000766.txta9000962.txt)\n",
      "(0.01,a9000117.txta9000927.txt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sorted = ShuffledRDD[1376] at sortByKey at <console>:161\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[1376] at sortByKey at <console>:161"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sorted = dup.rdd.map(d => (d(0) + d(3).toString, 1)).reduceByKey(_ + _).map(d => (d._2.toFloat/100, d._1)).sortByKey(false)\n",
    "sorted.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can work with LHS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
