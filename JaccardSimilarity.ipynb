{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileName1 = dataset_abstracts/a9000038.txt\n",
       "fileName2 = dataset_abstracts/a9000040.txt\n",
       "rows1 = MapPartitionsRDD[5756] at map at <console>:41\n",
       "text1 = principal investigator and associates. Topics in the followingareas are to be considered: (1) controlled Markov diffusionsand nonlinear PDEs; (2) asymptotic properties of nearlydeterministic Markov processes; (3) financial economicsapplications; (4) singular stochastic control; (5) computationalmethods in stochastic control; (6) stochastic calculus ofvariations; (7) nonlinear estimation. Analytical methods basedon viscosity solution techniques for nonlinear differentialequations as well as probabilistic methods will be studied.These theoretical studies are the basis for applied proble...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "principal investigator and associates. Topics in the followingareas are to be considered: (1) controlled Markov diffusionsand nonlinear PDEs; (2) asymptotic properties of nearlydeterministic Markov processes; (3) financial economicsapplications; (4) singular stochastic control; (5) computationalmethods in stochastic control; (6) stochastic calculus ofvariations; (7) nonlinear estimation. Analytical methods basedon viscosity solution techniques for nonlinear differentialequations as well as probabilistic methods will be studied.These theoretical studies are the basis for applied proble..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//input\n",
    "val fileName1 = \"dataset_abstracts/a9000038.txt\"\n",
    "val fileName2 = \"dataset_abstracts/a9000040.txt\"\n",
    "\n",
    "val rows1 = sc.textFile(fileName1).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text1 = rows1.reduce(_ + _)\n",
    "\n",
    "val rows2 = sc.textFile(fileName2).map(line=>line.trim().replaceAll(\"(\\\\s)+\", \" \")).cache()\n",
    "val text2 = rows2.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k = 5\n",
       "k_gram1 = List((323183978,\"\"), (879426833,\"\"), (207652440,\"\"), (1023262041,\"\"), (1191319030,\"\"), (-1029485329,\"\"), (94755376,\"\"), (798376504,\"\"), (-1163905680,\"\"), (982395043,\"\"), (-487295416,\"\"), (929914478,\"\"), (1998737565,\"\"), (2078865377,\"\"), (-1593058553,\"\"), (-81630221,\"\"), (1675405219,\"\"), (-1975782702,\"\"), (-312516106,\"\"), (-2146848343,\"\"), (899011444,\"\"), (-2008099792,\"\"), (1597990745,\"\"), (1159842749,\"\"), (2046055236,\"\"), (-1557845639,\"\"), (-2100210802,\"\"), (-206169076,\"\"), (-19109906,\"\"), (1127829784,\"\"), (-121854170,\"\"), (772595528,\"\"), (734667765,\"\"), (-1813676078,\"\"), (-708552550,\"\"), (-1949504179,\"\"), (-707004830,\"\"), (179624194,\"\"), (-1471358333,\"\"), (45318084,\"\"), (2014423945,\"\"), (-618699956,\"\"), (1272547154,\"\"), (240336297,\"\"), (9266125...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((323183978,\"\"), (879426833,\"\"), (207652440,\"\"), (1023262041,\"\"), (1191319030,\"\"), (-1029485329,\"\"), (94755376,\"\"), (798376504,\"\"), (-1163905680,\"\"), (982395043,\"\"), (-487295416,\"\"), (929914478,\"\"), (1998737565,\"\"), (2078865377,\"\"), (-1593058553,\"\"), (-81630221,\"\"), (1675405219,\"\"), (-1975782702,\"\"), (-312516106,\"\"), (-2146848343,\"\"), (899011444,\"\"), (-2008099792,\"\"), (1597990745,\"\"), (1159842749,\"\"), (2046055236,\"\"), (-1557845639,\"\"), (-2100210802,\"\"), (-206169076,\"\"), (-19109906,\"\"), (1127829784,\"\"), (-121854170,\"\"), (772595528,\"\"), (734667765,\"\"), (-1813676078,\"\"), (-708552550,\"\"), (-1949504179,\"\"), (-707004830,\"\"), (179624194,\"\"), (-1471358333,\"\"), (45318084,\"\"), (2014423945,\"\"), (-618699956,\"\"), (1272547154,\"\"), (240336297,\"\"), (9266125..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//k-gram\n",
    "val k = 5\n",
    "val k_gram1 = text1.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD1 = sc.parallelize(k_gram1)\n",
    "val filtered1 = kgramRDD1.distinct()\n",
    "\n",
    "val k_gram2 = text2.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\"))\n",
    "val kgramRDD2 = sc.parallelize(k_gram2)\n",
    "val filtered2 = kgramRDD2.distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6188993\n",
      "0.3811007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join = MapPartitionsRDD[5770] at leftOuterJoin at <console>:40\n",
       "join_n = 1192.0\n",
       "union = MapPartitionsRDD[5774] at distinct at <console>:42\n",
       "union_n = 1926.0\n",
       "sim = 0.6188993\n",
       "jacc_dist = 0.3811007\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3811007"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val join = filtered1.leftOuterJoin(filtered2)\n",
    "val join_n = join.count().toFloat\n",
    "val union = filtered1.union(filtered2).distinct()\n",
    "val union_n = union.count().toFloat\n",
    "\n",
    "val sim = join_n / union_n\n",
    "val jacc_dist = 1-sim\n",
    "\n",
    "println(sim)\n",
    "println(jacc_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with Min-hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:55: error: type mismatch;\n",
       " found   : org.apache.spark.rdd.RDD[List[(Int, String)]]\n",
       " required: Seq[?]\n",
       "Error occurred in an application involving default arguments.\n",
       "       val kgramRDD = sc.parallelize(k_gram1)\n",
       "                                     ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "val dir = new File(\"dataset_abstracts\")\n",
    "val files = dir.listFiles() //379\n",
    "val maxSize = 10\n",
    "val docSize = if(files.size < maxSize) files.size else maxSize\n",
    "\n",
    "val docs = files.map(f => sc.textFile(f.getPath() ).map(line=>line.replaceAll(\"(\\\\s)+\", \" \")).collect().reduce(_ + _))\n",
    "val docsRDD = sc.parallelize(docs).cache()\n",
    "val abstracts = docsRDD.map(doc => doc.split(\"Abstract :\")).filter(a => a.size > 1).map(a => a(1))\n",
    "\n",
    "//TODO RDD[ RDD [String]] --> RDD[(docId, shingle)]\n",
    "val k = 5\n",
    "val k_gram1 = abstracts.map(a => a.split(\"\").sliding(k).toList.map(x => (scala.util.hashing.MurmurHash3.arrayHash(x), \"\")))\n",
    "val kgramRDD = sc.parallelize(k_gram1)\n",
    "val filtered = kgramRDD.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p = 1073676287\n",
       "m = 1073676288\n",
       "doc_signature = Array(-701099952, -490456252, -819415235, -982796781, -972365587, -539719326, -991711111, -861865276, -954276687, -936137481, -636549802, -291813556, -958014372, -751095575, -226580326, -298989585, -934393359, -761294500, -471033875, -564140757, -786245149, -853592562, -256835389, -955319261, -234031736, -885353197, -422056576, -685021830, -1053303560, -801120782, -538153782, -901692924, -1032942575, -425691361, -263268863, -928073791, -844671812, -845859844, -691309734, -846536457, -1019873399, -456506007, -890476283, -635220461, -700645489, -827812163, -348668524, -727526301, -328853667, -977040893, -933394679, -891249220, -936442531, -636264737, -90962801, -1022817264, -1035...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hashThis: (a: Long, b: Long, x: Long)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(-701099952, -490456252, -819415235, -982796781, -972365587, -539719326, -991711111, -861865276, -954276687, -936137481, -636549802, -291813556, -958014372, -751095575, -226580326, -298989585, -934393359, -761294500, -471033875, -564140757, -786245149, -853592562, -256835389, -955319261, -234031736, -885353197, -422056576, -685021830, -1053303560, -801120782, -538153782, -901692924, -1032942575, -425691361, -263268863, -928073791, -844671812, -845859844, -691309734, -846536457, -1019873399, -456506007, -890476283, -635220461, -700645489, -827812163, -348668524, -727526301, -328853667, -977040893, -933394679, -891249220, -936442531, -636264737, -90962801, -1022817264, -1035..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Min-hashing\n",
    "\n",
    "val p = 1073676287\n",
    "val m = p + 1\n",
    "def hashThis(a:Long, b:Long, x:Long): Long = {\n",
    "    ((a*x + b) % p ) % m\n",
    "}\n",
    "\n",
    "//TODO: Outer loop: foreach document in set of documents\n",
    "var doc_signature:Array[Long] = new Array[Long](100)\n",
    "for(i<-0 to 99){\n",
    "    \n",
    "    val r = new scala.util.Random(i)\n",
    "    val a2 = r.nextInt(p-1)\n",
    "    val a = if(a2%2==0) a2+1 else a2\n",
    "    val b = r.nextInt(p-1)\n",
    "    \n",
    "    //TODO: Iterate through docs\n",
    "    //for(d in doc){\n",
    "    //    val min = d.minhashThis()\n",
    "    //}\n",
    "    \n",
    "    val min = filtered1.map(s => s._1.toLong).reduce((x,y) => hashThis(a, b, x) min hashThis(a, b, y) )\n",
    "    \n",
    "    doc_signature(i) = min\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
