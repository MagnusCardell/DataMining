{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:12: error: '.' expected but ';' found.\n",
       "case class item(items: Array[Int])\n",
       "^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions.countDistinct\n",
    "import org.apache.spark.sql.functions.collect_list\n",
    "import org.apache.spark.sql.functions.count\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.Row\n",
    "import itertools\n",
    "\n",
    "case class item(items: Array[Int])\n",
    "case class candidate(value: Array[Int])\n",
    "\n",
    "val dataRDD = sc.textFile(\"T10I4D100K.dat\").map(row => row.split(\" \").map(v => v.toInt).toArray).map(d => item(d))\n",
    "val dataCount = dataRDD.count()\n",
    "val support = 0.01 * dataCount\n",
    "\n",
    "//rawDataRDD.collect.foreach(println)\n",
    "//println(dataCount)\n",
    "//println(support)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val itemsRDD = dataRDD.zipWithIndex().map(d => item(d._2, d._1))\n",
    "//println(itemsRDD.take(1)(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidates = MapPartitionsRDD[61] at map at <console>:114\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[61] at map at <console>:114"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidates = dataRDD.map(row => row.items.map(item => (item) ))\n",
    "    .flatMap(y => y)\n",
    "    .distinct\n",
    "    .map(c => candidate(Array(c)))\n",
    "println(candidates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| [68]|\n",
      "|[454]|\n",
      "|[324]|\n",
      "|[180]|\n",
      "|[320]|\n",
      "|[752]|\n",
      "|[408]|\n",
      "|[428]|\n",
      "|[986]|\n",
      "|[464]|\n",
      "|[346]|\n",
      "| [14]|\n",
      "|[466]|\n",
      "| [24]|\n",
      "|[520]|\n",
      "|[912]|\n",
      "|[146]|\n",
      "|[140]|\n",
      "|[204]|\n",
      "|[514]|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|               items|value|\n",
      "+--------------------+-----+\n",
      "|[25, 52, 164, 240...| [68]|\n",
      "|[25, 52, 164, 240...|[454]|\n",
      "|[25, 52, 164, 240...|[324]|\n",
      "|[25, 52, 164, 240...|[180]|\n",
      "|[25, 52, 164, 240...|[320]|\n",
      "|[25, 52, 164, 240...|[752]|\n",
      "|[25, 52, 164, 240...|[408]|\n",
      "|[25, 52, 164, 240...|[428]|\n",
      "|[25, 52, 164, 240...|[986]|\n",
      "|[25, 52, 164, 240...|[464]|\n",
      "|[25, 52, 164, 240...|[346]|\n",
      "|[25, 52, 164, 240...| [14]|\n",
      "|[25, 52, 164, 240...|[466]|\n",
      "|[25, 52, 164, 240...| [24]|\n",
      "|[25, 52, 164, 240...|[520]|\n",
      "|[25, 52, 164, 240...|[912]|\n",
      "|[25, 52, 164, 240...|[146]|\n",
      "|[25, 52, 164, 240...|[140]|\n",
      "|[25, 52, 164, 240...|[204]|\n",
      "|[25, 52, 164, 240...|[514]|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+---------+\n",
      "|               items|value|singleton|\n",
      "+--------------------+-----+---------+\n",
      "|[25, 52, 164, 240...| [68]|       []|\n",
      "|[25, 52, 164, 240...|[454]|       []|\n",
      "|[25, 52, 164, 240...|[324]|       []|\n",
      "|[25, 52, 164, 240...|[180]|       []|\n",
      "|[25, 52, 164, 240...|[320]|       []|\n",
      "|[25, 52, 164, 240...|[752]|       []|\n",
      "|[25, 52, 164, 240...|[408]|       []|\n",
      "|[25, 52, 164, 240...|[428]|       []|\n",
      "|[25, 52, 164, 240...|[986]|       []|\n",
      "|[25, 52, 164, 240...|[464]|       []|\n",
      "|[25, 52, 164, 240...|[346]|       []|\n",
      "|[25, 52, 164, 240...| [14]|       []|\n",
      "|[25, 52, 164, 240...|[466]|       []|\n",
      "|[25, 52, 164, 240...| [24]|       []|\n",
      "|[25, 52, 164, 240...|[520]|       []|\n",
      "|[25, 52, 164, 240...|[912]|       []|\n",
      "|[25, 52, 164, 240...|[146]|       []|\n",
      "|[25, 52, 164, 240...|[140]|       []|\n",
      "|[25, 52, 164, 240...|[204]|       []|\n",
      "|[25, 52, 164, 240...|[514]|       []|\n",
      "+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|               items|singleton|\n",
      "+--------------------+---------+\n",
      "|[25, 52, 164, 240...|    [834]|\n",
      "|[25, 52, 164, 240...|    [164]|\n",
      "|[25, 52, 164, 240...|    [368]|\n",
      "|[25, 52, 164, 240...|    [730]|\n",
      "|[25, 52, 164, 240...|     [52]|\n",
      "|[25, 52, 164, 240...|    [240]|\n",
      "|[25, 52, 164, 240...|    [630]|\n",
      "|[25, 52, 164, 240...|    [274]|\n",
      "|[25, 52, 164, 240...|    [538]|\n",
      "|[25, 52, 164, 240...|    [328]|\n",
      "|[25, 52, 164, 240...|    [448]|\n",
      "|[39, 120, 124, 20...|    [814]|\n",
      "|[39, 120, 124, 20...|    [834]|\n",
      "|[39, 120, 124, 20...|    [124]|\n",
      "|[39, 120, 124, 20...|    [120]|\n",
      "|[39, 120, 124, 20...|    [704]|\n",
      "|[35, 249, 674, 71...|    [712]|\n",
      "|[35, 249, 674, 71...|    [854]|\n",
      "|[35, 249, 674, 71...|    [674]|\n",
      "|[35, 249, 674, 71...|    [950]|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-------+\n",
      "|singleton|support|\n",
      "+---------+-------+\n",
      "|    [496]|   1428|\n",
      "|    [471]|   2894|\n",
      "|    [392]|   2420|\n",
      "|    [540]|   1293|\n",
      "|    [623]|   1845|\n",
      "|    [897]|   1935|\n",
      "|    [516]|   1544|\n",
      "|     [31]|   1666|\n",
      "|    [580]|   1667|\n",
      "|     [85]|   1555|\n",
      "|    [458]|   1124|\n",
      "|    [883]|   4902|\n",
      "|    [970]|   2086|\n",
      "|    [804]|   1315|\n",
      "|    [296]|   2210|\n",
      "|    [472]|   2125|\n",
      "|    [853]|   1804|\n",
      "|     [78]|   2471|\n",
      "|    [918]|   3012|\n",
      "|    [322]|   1154|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidatesDF = [value: array<int>]\n",
       "dataDF = [items: array<int>]\n",
       "crossDF = [items: array<int>, value: array<int>]\n",
       "singletonDF = [items: array<int>, value: array<int> ... 1 more field]\n",
       "sqlContext = org.apache.spark.sql.SQLContext@63811d0b\n",
       "frequent_singletons = [items: array<int>, singleton: array<int>]\n",
       "freq_singletons = [singleton: array<int>, support: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[singleton: array<int>, support: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidatesDF = candidates.toDF()\n",
    "val dataDF = dataRDD.toDF()\n",
    "val crossDF = dataDF.crossJoin(candidatesDF)\n",
    "candidatesDF.show()\n",
    "crossDF.show()\n",
    "\n",
    "val singletonDF = crossDF.withColumn(\n",
    "    \"singleton\",\n",
    "    array_intersect( col(\"value\"), col(\"items\"))\n",
    ")\n",
    "singletonDF.show()\n",
    "singletonDF.createOrReplaceTempView(\"singletons\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val frequent_singletons = sqlContext.sql(\"SELECT d.items, d.singleton FROM singletons d WHERE size(d.singleton) > 0 \")\n",
    "frequent_singletons.show()\n",
    "\n",
    "val freq_singletons = frequent_singletons.groupBy(\"singleton\").agg(count(\"items\")as \"support\").where(col(\"support\") > support)\n",
    "freq_singletons.show\n",
    "\n",
    "//println(dup_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|singleton|singleton2|\n",
      "+---------+----------+\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job 82 cancelled as part of cancellation of all jobs\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2082)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:713)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val candidate_2a = freq_singletons.select(\"singleton\")\n",
    "val candidate_2 = candidate_2a.crossJoin(candidate_2a.select(col(\"singleton\") as \"singleton2\") where \"singleton < singleton2\")\n",
    "//TODO\n",
    "candidate_2.show\n",
    "\n",
    "val crossDF2 = candidate_2.withColumn(\n",
    "    \"candidate2\",\n",
    "    concat(col(\"singleton\"), col(\"singleton2\"))\n",
    ").crossJoin(dataDF).drop(\"singleton\").drop(\"singleton2\")\n",
    "crossDF2.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+\n",
      "|candidate2|               items|dupleton|\n",
      "+----------+--------------------+--------+\n",
      "|[496, 496]|[25, 52, 164, 240...|      []|\n",
      "|[496, 496]|[39, 120, 124, 20...|      []|\n",
      "|[496, 496]|[35, 249, 674, 71...|      []|\n",
      "|[496, 496]|[39, 422, 449, 70...|      []|\n",
      "|[496, 496]|[15, 229, 262, 28...|      []|\n",
      "|[496, 496]|[26, 104, 143, 32...|      []|\n",
      "|[496, 496]|[7, 185, 214, 350...|      []|\n",
      "|[496, 496]|          [227, 390]|      []|\n",
      "|[496, 496]|[71, 192, 208, 27...|   [496]|\n",
      "|[496, 496]|[183, 193, 217, 2...|   [496]|\n",
      "|[496, 496]|[161, 175, 177, 4...|      []|\n",
      "|[496, 496]|[125, 130, 327, 6...|      []|\n",
      "|[496, 496]|[392, 461, 569, 8...|      []|\n",
      "|[496, 496]|[27, 78, 104, 177...|      []|\n",
      "|[496, 496]|[101, 147, 229, 3...|      []|\n",
      "|[496, 496]|[71, 208, 217, 26...|      []|\n",
      "|[496, 496]|[43, 70, 176, 204...|      []|\n",
      "|[496, 496]|  [25, 52, 278, 730]|      []|\n",
      "|[496, 496]|[151, 432, 504, 8...|      []|\n",
      "|[496, 496]|[71, 73, 118, 274...|      []|\n",
      "+----------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------+\n",
      "|               items|dupleton|\n",
      "+--------------------+--------+\n",
      "|[71, 192, 208, 27...|   [496]|\n",
      "|[183, 193, 217, 2...|   [496]|\n",
      "|[376, 392, 496, 5...|   [496]|\n",
      "|[71, 192, 196, 27...|   [496]|\n",
      "|[2, 33, 177, 193,...|   [496]|\n",
      "|[32, 71, 192, 272...|   [496]|\n",
      "|[43, 75, 129, 140...|   [496]|\n",
      "|[72, 96, 183, 186...|   [496]|\n",
      "|[129, 279, 496, 6...|   [496]|\n",
      "|[10, 129, 192, 27...|   [496]|\n",
      "|[346, 368, 435, 4...|   [496]|\n",
      "|[132, 298, 318, 4...|   [496]|\n",
      "|[25, 68, 496, 524...|   [496]|\n",
      "|[26, 100, 129, 17...|   [496]|\n",
      "|[33, 217, 283, 34...|   [496]|\n",
      "| [71, 272, 300, 496]|   [496]|\n",
      "|[145, 354, 422, 4...|   [496]|\n",
      "|[37, 40, 94, 112,...|   [496]|\n",
      "|[51, 70, 73, 139,...|   [496]|\n",
      "|[177, 496, 579, 6...|   [496]|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job 68 cancelled as part of cancellation of all jobs\n",
       "StackTrace: org.apache.spark.SparkException: Job 68 cancelled as part of cancellation of all jobs\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)\n",
       "  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:830)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2082)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:713)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dupletonDF = crossDF2.withColumn(\n",
    "    \"dupleton\",\n",
    "    array_intersect( col(\"candidate2\"), col(\"items\"))\n",
    ")\n",
    "dupletonDF.show()\n",
    "dupletonDF.createOrReplaceTempView(\"dupletons\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val frequent_dupl = sqlContext.sql(\"SELECT d.items, d.dupleton FROM dupletons d WHERE size(d.dupleton) > 0 \")\n",
    "frequent_dupl.show()\n",
    "\n",
    "val freq_dupletons = frequent_dupl.groupBy(\"dupleton\").agg(count(\"items\")as \"support\").where(col(\"support\") > support)\n",
    "freq_dupletons.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//archive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//k = 1\n",
    "val k = 1\n",
    "val singleton_c = dataRDD.map(r=> r.map(item => (item, 1)))\n",
    "    .flatMap(y=>y).reduceByKey(_ + _)\n",
    "    .map(d => candidate(d._1, d._2))\n",
    "val singleton = singleton_c.filter(s => s.support >= support)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val k_ = 2\n",
    "//val dupletons = singleton.map(s => s.value).collect.toSet.subsets(k_).copyToArray(d2)\n",
    "case class candidate2(value: Array[Int], support: Int)\n",
    "val candidat = singleton.map(s => candidate2(Array(s.value), s.support))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val singletonDF = spark.createDataFrame(singleton) \n",
    "val itemsDF = spark.createDataFrame(itemsRDD)\n",
    "//itemsDF.show(200)\n",
    "\n",
    "singletonDF.createOrReplaceTempView(\"single\")\n",
    "itemsDF.createOrReplaceTempView(\"data\")\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val dup = sqlContext.sql(\"SELECT s1.value as v1, s2.value as v2, d.items, d.row FROM data d, single s1, single s2 WHERE s1.value != s2.value AND array_contains(d.items, s1.value) AND array_contains(d.items, s2.value)\")\n",
    "dup.show()\n",
    "println(dup.count())\n",
    "\n",
    "\n",
    "val result = dup.rdd.map(d => ((d(0), d(1)), 1)).reduceByKey(_ + _).filter(d => d._2 > support)\n",
    "result.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "\n",
    "val singleDF = spark.createDataFrame(candidat) \n",
    "singleDF.show()\n",
    "\n",
    "val crossDF = itemsDF.crossJoin(singleDF)\n",
    "crossDF.show()\n",
    "\n",
    "val dupleDF = crossDF.withColumn(\n",
    "    \"singleton\",\n",
    "    array_intersect( col(\"value\"), col(\"items\"))\n",
    ")\n",
    "dupleDF.show()\n",
    "dupleDF.createOrReplaceTempView(\"dupz\")\n",
    "val dup_filtered = sqlContext.sql(\"SELECT d.items, d.row, d.singleton FROM dupz d WHERE size(d.singleton) > 0 \")\n",
    "dup_filtered.show()\n",
    "\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "dup_filtered.groupBy(\"singleton\").agg(count(\"row\")as \"support\").where(col(\"support\") > support).show\n",
    "\n",
    "//println(dup_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCandidates(row: Set[Int], candidates: Set[Int], k: Int): Set[(Set[Int],Int)] = {\n",
    "    val possible = candidates.subsets(k)\n",
    "    val c = possible.map(p => if(p.subsetOf(row)) (p,1) else (p,0) ).toSet\n",
    "    c\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "val data = dataRDD.map(d => generateCandidates(d, candidates, 2))\n",
    "data.collect.foreach(println)\n",
    "//apriori_k(1, support, candidates, dataRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_k(k: Int, s: Float, candidates: RDD[Int], data: Any): Any = {\n",
    "    if( candidates.size < 2) return data\n",
    "    \n",
    "    C_t = generateCandidates(k, candidates, s)\n",
    "    println(k)\n",
    "    apriori_k(k+1, s, sc.parallelize(candidates.take((candidates.count()-1).toInt)), data ++ C_t)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
